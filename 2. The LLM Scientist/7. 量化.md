## 量化是使用较低精度转换模型的权重（和激活）的过程
例如，使用 16 位存储的权重可以转换为 4 位表示。这种技术对于降低与 LLM 相关的计算和内存成本变得越来越重要。


* **基本技术**: 了解不同级别的精度（FP32、FP16、INT8 等）以及如何使用 absmax 和零点技术执行朴素量化
* **GGUF and llama.cpp**: [llama.cpp](https://github.com/ggerganov/llama.cpp)  和 GGUF 格式最初设计为在 CPU 上运行，现已成为在消费级硬件上运行 LLM 的最流行工具
* **GPTQ and EXL2**: [GPTQ](https://arxiv.org/abs/2210.17323) 更具体地说， [EXL2](https://github.com/turboderp/exllamav2)  格式提供了令人难以置信的速度，但只能在 GPU 上运行，模型也需要很长时间才能被量化
* **AWQ**: 这种新格式比 GPTQ 更准确（更低的困惑度），但使用的 VRAM 要多得多，而且不一定更快

📚 **推荐课程**:
* [量化简介](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html): 量化、绝对最大值和零点量化以及带有代码的 LLM.int8（） 概述
* [使用 llama.cpp 量化 Llama 模型](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html): 有关如何使用 llama.cpp 和 GGUF 格式量化 Llama 2 模型的教程
* [使用 GPTQ 进行 4 位 LLM 量化](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html): 有关如何使用 GPTQ 算法和 AutoGPTQ 量化 LLM 的教程
* [ExLlamaV2：运行 LLM 的最快库](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html):有关如何使用 EXL2 格式量化 Mistral 模型并使用 ExLlamaV2 库运行模型的指南
* [了解激活感知权重量化](https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8) by FriendliAI: AWQ 技术及其优势概述

