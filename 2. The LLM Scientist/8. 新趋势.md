## 未来趋势

* **Positional embeddings**: 学习大型语言模型如何编码位置，特别是像相对位置编码方案[RoPE](https://arxiv.org/abs/2104.09864)这样的方法。实现[YaRN](https://arxiv.org/abs/2309.00071)（将注意力矩阵乘以一个温度因子）或[ALiBi](https://arxiv.org/abs/2108.12409)（基于标记距离的注意力惩罚）以扩展上下文长度
* **Model merging**: 合并训练模型已成为在不进行任何微调的情况下创建高性能模型的流行方法。流行的 [mergekit](https://github.com/cg123/mergekit) 库实现了最流行的合并方法，如 SLERP、[DARE](https://arxiv.org/abs/2311.03099) 和 [TIES](https://arxiv.org/abs/2311.03099)
* **Mixture of Experts**: [Mixtral](https://arxiv.org/abs/2401.04088) 因其出色的性能重新普及了 MoE 架构。与此同时，一种称为 frankenMoE 的模型在 OSS 社区中出现，通过合并像 [Phixtral](https://huggingface.co/mlabonne/phixtral-2x2_8) 这样的模型，它成为了一种更便宜且性能良好的选择
* **Multimodal models**: 这些模型(如[CLIP](https://openai.com/research/clip)、[Stable Diffusion](https://stability.ai/stable-image)或[LLaVA](https://llava-vl.github.io/))通过统一的嵌入空间处理多种类型的输入（文本、图像、音频等），这解锁了强大的应用，如文本到图像

📚 **推荐课程**:
* [扩展 RoPE](https://blog.eleuther.ai/yarn/) by EleutherAI: 总结不同位置编码技术的文章
* [了解 YaRN](https://medium.com/@rcrajatchawla/understanding-yarn-extending-context-window-of-llms-3f21e3522465) by Rajat Chawla: YaRN 简介
* [使用 mergekit 合并 LLM](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html): 有关使用 mergekit 进行模型合并的教程
* [混合专家模型](https://huggingface.co/blog/moe) by Hugging Face: 有关 MoE 及其工作原理的详尽指南
* [大型多模态模型](https://huyenchip.com/2023/10/10/multimodal.html) by Chip Huyen: 多模态系统概述和该领域的近期历史 
