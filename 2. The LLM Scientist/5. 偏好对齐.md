## åœ¨ç›‘ç£å¾®è°ƒä¹‹åï¼ŒRLHF æ˜¯ç”¨äºå°† LLM çš„ç­”æ¡ˆä¸äººç±»æœŸæœ›ä¿æŒä¸€è‡´çš„ä¸€ä¸ªæ­¥éª¤
ä»äººç±»ï¼ˆæˆ–äººå·¥ï¼‰åé¦ˆä¸­å­¦ä¹ åå¥½ï¼Œè¿™å¯ç”¨äºå‡å°‘åè§ã€å®¡æŸ¥æ¨¡å‹æˆ–ä½¿å®ƒä»¬ä»¥æ›´æœ‰ç”¨çš„æ–¹å¼è¿è¡Œã€‚å®ƒæ¯” SFT æ›´å¤æ‚ï¼Œé€šå¸¸è¢«è§†ä¸ºå¯é€‰ã€‚

* **Preference datasets**: è¿™äº›æ•°æ®é›†é€šå¸¸åŒ…å«å¤šä¸ªå…·æœ‰æŸç§æ’åçš„ç­”æ¡ˆï¼Œè¿™ä½¿å¾—å®ƒä»¬æ¯”æŒ‡ä»¤æ•°æ®é›†æ›´éš¾ç”Ÿæˆ
* [**Proximal Policy Optimization**](https://arxiv.org/abs/1707.06347): PPO åˆ©ç”¨å¥–åŠ±æ¨¡å‹æ¥é¢„æµ‹ç»™å®šæ–‡æœ¬æ˜¯å¦è¢«äººç±»é«˜åº¦æ’åã€‚ç„¶åï¼Œæ­¤é¢„æµ‹ç”¨äºä¼˜åŒ– SFT æ¨¡å‹ï¼Œå¹¶æ ¹æ® KL æ•£åº¦è¿›è¡Œæƒ©ç½š
* **[Direct Preference Optimization](https://arxiv.org/abs/2305.18290)**: DPO é€šè¿‡å°†æµç¨‹é‡æ–°å®šä¹‰ä¸ºåˆ†ç±»é—®é¢˜æ¥ç®€åŒ–æµç¨‹ã€‚å®ƒä½¿ç”¨å‚è€ƒæ¨¡å‹è€Œä¸æ˜¯å¥–åŠ±æ¨¡å‹ï¼ˆæ— éœ€è®­ç»ƒï¼‰ï¼Œå¹¶ä¸”åªéœ€è¦ä¸€ä¸ªè¶…å‚æ•°ï¼Œä½¿å…¶æ›´åŠ ç¨³å®šå’Œé«˜æ•ˆ

ğŸ“š **æ¨èè¯¾ç¨‹**:
* [Distilabel](https://github.com/argilla-io/distilabel) by Argilla: åˆ›å»ºæ‚¨è‡ªå·±çš„æ•°æ®é›†çš„ç»ä½³å·¥å…·ã€‚å®ƒæ˜¯ä¸“ä¸ºåå¥½æ•°æ®é›†è®¾è®¡çš„ï¼Œä½†ä¹Ÿå¯ä»¥æ‰§è¡Œ SFT
* [ä½¿ç”¨ RLHF è®­ç»ƒ LLM ](https://wandb.ai/ayush-thakur/Intro-RLAIF/reports/An-Introduction-to-Training-LLMs-Using-Reinforcement-Learning-From-Human-Feedback-RLHF---VmlldzozMzYyNjcy) by Ayush Thakur: è§£é‡Šä¸ºä»€ä¹ˆ RLHF éœ€è¦å‡å°‘ LLM ä¸­çš„åå·®å¹¶æé«˜æ€§èƒ½
* [Illustration RLHF](https://huggingface.co/blog/rlhf) by Hugging Face: RLHF ç®€ä»‹ï¼ŒåŒ…æ‹¬å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒ
* [Preference Tuning LLMs](https://huggingface.co/blog/pref-tuning) by Hugging Face: æ¯”è¾ƒ DPOã€IPO å’Œ KTO ç®—æ³•ä»¥æ‰§è¡Œé¦–é€‰é¡¹å¯¹é½
* [LLM è®­ç»ƒï¼šRLHF åŠå…¶æ›¿ä»£æ–¹æ¡ˆ](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives) by Sebastian Rashcka: RLHF æµç¨‹å’Œ RLAIF ç­‰æ›¿ä»£æ–¹æ¡ˆæ¦‚è¿°
* [ä½¿ç”¨ DPO å¾®è°ƒ Mistral-7b](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html): ä½¿ç”¨ DPO å¾®è°ƒ Mistral-7b æ¨¡å‹å¹¶é‡ç° [NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B) çš„æ•™ç¨‹ã€‚
