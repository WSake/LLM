## 大规模部署LLM是一个可能需要多个GPU集群的工程
在其他情况下，演示和本地应用程序可以以低得多的复杂度实现

* **Local deployment**: 隐私是开源大型语言模型相较于私有模型的重要优势。本地LLM服务器（如[LM Studio](https://lmstudio.ai/)、[Ollama](https://ollama.ai/)、[oobabooga](https://github.com/oobabooga/text-generation-webui)、[kobold.cpp](https://github.com/LostRuins/koboldcpp)等）利用这一优势为本地应用程序提供支持 
* **Demo deployment**: 像[Gradio](https://www.gradio.app/)和[Streamlit](https://docs.streamlit.io/)这样的框架有助于原型设计应用程序和分享演示。您还可以轻松地将它们托管在线上，例如使用[Hugging Face Spaces](https://huggingface.co/spaces)
* **Server deployment**: 大规模部署LLM需要云（另见[SkyPilot](https://skypilot.readthedocs.io/en/latest/)）或本地基础设施，并经常利用优化的文本生成框架，如[TGI](https://github.com/huggingface/text-generation-inference)、[vLLM](https://github.com/vllm-project/vllm/tree/main)等
* **Edge deployment**: 在受限环境下，高性能框架如[MLC LLM](https://github.com/mlc-ai/mlc-llm)和[mnn-llm](https://github.com/wangzhaode/mnn-llm/blob/master/README_en.md)可以在网页浏览器、Android和iOS上部署大型语言模型 

* 📚 **推荐课程**:
* [Streamlit - Build a basic LLM app](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps): 使用Streamlit创建基本的类似ChatGPT应用程序的教程
* [HF LLM Inference Container](https://huggingface.co/blog/sagemaker-huggingface-llm): 在Amazon SageMaker上使用Hugging Face的推理容器部署LLM
* [Philschmid blog](https://www.philschmid.de/) by Philipp Schmid: 关于使用Amazon SageMaker部署大型语言模型的高质量文章集合
* [Optimizing latence](https://hamel.dev/notes/llm/inference/03_inference.html) by Hamel Husain: 在吞吐量和延迟方面的TGI、vLLM、CTranslate2和mlc的比较 




