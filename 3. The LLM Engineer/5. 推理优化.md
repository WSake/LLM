## 文本生成是一个昂贵的过程，需要昂贵的硬件，除了量化之外，还提出了各种技术来最大化吞吐量和减少推理成本

* **Flash Attention**: 优化注意力机制，将其复杂度从二次变为线性，从而加速训练和推理
* **Key-value cache**: 理解键值缓存以及在[多查询注意力机制](https://arxiv.org/abs/1911.02150)（MQA）和[分组查询注意力机制](https://arxiv.org/abs/2305.13245)（GQA）中引入的改进
* **Speculative decoding**: 使用小型模型生成草稿，然后由较大模型进行审查以加速文本生成 

  
📚 **推荐课程**:
* [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one) by Hugging Face: 解释如何优化GPU上的推理
* [LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) by Databricks: 如何优化生产环境中LLM推理的最佳实践
* [Optimizing LLMs for Speed and Memory](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization) by Hugging Face: 解释三种优化速度和内存的主要技术，即量化、Flash Attention（闪存注意力）和架构创新
* [Assisted Generation](https://huggingface.co/blog/assisted-generation) by Hugging Face: HF版本的推测解码，这是一篇有趣的博客文章，介绍了它是如何工作的以及实现它的代码



