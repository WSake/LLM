## æ–‡æœ¬ç”Ÿæˆæ˜¯ä¸€ä¸ªæ˜‚è´µçš„è¿‡ç¨‹ï¼Œéœ€è¦æ˜‚è´µçš„ç¡¬ä»¶ï¼Œé™¤äº†é‡åŒ–ä¹‹å¤–ï¼Œè¿˜æå‡ºäº†å„ç§æŠ€æœ¯æ¥æœ€å¤§åŒ–ååé‡å’Œå‡å°‘æ¨ç†æˆæœ¬

* **Flash Attention**: ä¼˜åŒ–æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†å…¶å¤æ‚åº¦ä»äºŒæ¬¡å˜ä¸ºçº¿æ€§ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†
* **Key-value cache**: ç†è§£é”®å€¼ç¼“å­˜ä»¥åŠåœ¨[å¤šæŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶](https://arxiv.org/abs/1911.02150)ï¼ˆMQAï¼‰å’Œ[åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶](https://arxiv.org/abs/2305.13245)ï¼ˆGQAï¼‰ä¸­å¼•å…¥çš„æ”¹è¿›
* **Speculative decoding**: ä½¿ç”¨å°å‹æ¨¡å‹ç”Ÿæˆè‰ç¨¿ï¼Œç„¶åç”±è¾ƒå¤§æ¨¡å‹è¿›è¡Œå®¡æŸ¥ä»¥åŠ é€Ÿæ–‡æœ¬ç”Ÿæˆ 

  
ğŸ“š **æ¨èè¯¾ç¨‹**:
* [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one) by Hugging Face: è§£é‡Šå¦‚ä½•ä¼˜åŒ–GPUä¸Šçš„æ¨ç†
* [LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) by Databricks: å¦‚ä½•ä¼˜åŒ–ç”Ÿäº§ç¯å¢ƒä¸­LLMæ¨ç†çš„æœ€ä½³å®è·µ
* [Optimizing LLMs for Speed and Memory](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization) by Hugging Face: è§£é‡Šä¸‰ç§ä¼˜åŒ–é€Ÿåº¦å’Œå†…å­˜çš„ä¸»è¦æŠ€æœ¯ï¼Œå³é‡åŒ–ã€Flash Attentionï¼ˆé—ªå­˜æ³¨æ„åŠ›ï¼‰å’Œæ¶æ„åˆ›æ–°
* [Assisted Generation](https://huggingface.co/blog/assisted-generation) by Hugging Face: HFç‰ˆæœ¬çš„æ¨æµ‹è§£ç ï¼Œè¿™æ˜¯ä¸€ç¯‡æœ‰è¶£çš„åšå®¢æ–‡ç« ï¼Œä»‹ç»äº†å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ä»¥åŠå®ç°å®ƒçš„ä»£ç 



