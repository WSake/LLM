## 除了与软件相关的传统安全问题外，LLM由于其训练和提示方式具有独特的弱点

* **Prompt hacking**: 与提示工程相关的不同技术，包括提示注入（附加指令以确保模型的答案）、数据/提示泄露（检索其原始数据/提示）和越狱（设计提示以绕过安全功能）
* **Backdoors**: 攻击向量可以针对训练数据本身，通过污染训练数据（例如，使用虚假信息）或创建后门（秘密触发器以在推理期间改变模型的行为）
* **Defensive measures**: 保护你的大型语言模型应用的最佳方式是通过针对这些漏洞进行测试（例如，使用红队测试和像[garak](https://github.com/leondz/garak/)这样的检查工具），并在生产环境中观察它们（使用像[langfuse](https://github.com/langfuse/langfuse/)这样的框架）

📚 **推荐课程**:
* [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) by HEGO Wiki: LLM应用程序中出现的10个最关键漏洞列表
* [Prompt Injection Primer](https://github.com/jthack/PIPE) by Joseph Thacker: 简短指南致力于为工程师提供prompt注入
* [LLM Security](https://llmsecurity.net/) by [@llm_sec](https://twitter.com/llm_sec): 与LLM安全相关的大量资源列表
* [Red teaming LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming) by Microsoft: 使用LLM进行演练的指南
---



