## 在监督微调之后，RLHF 是用于将 LLM 的答案与人类期望保持一致的一个步骤
从人类（或人工）反馈中学习偏好，这可用于减少偏见、审查模型或使它们以更有用的方式运行。它比 SFT 更复杂，通常被视为可选。

* **Preference datasets**: 这些数据集通常包含多个具有某种排名的答案，这使得它们比指令数据集更难生成
* [**Proximal Policy Optimization**](https://arxiv.org/abs/1707.06347): PPO 利用奖励模型来预测给定文本是否被人类高度排名。然后，此预测用于优化 SFT 模型，并根据 KL 散度进行惩罚
* **[Direct Preference Optimization](https://arxiv.org/abs/2305.18290)**: DPO 通过将流程重新定义为分类问题来简化流程。它使用参考模型而不是奖励模型（无需训练），并且只需要一个超参数，使其更加稳定和高效

📚 **推荐课程**:
* [Distilabel](https://github.com/argilla-io/distilabel) by Argilla: 创建您自己的数据集的绝佳工具。它是专为偏好数据集设计的，但也可以执行 SFT
* [使用 RLHF 训练 LLM ](https://wandb.ai/ayush-thakur/Intro-RLAIF/reports/An-Introduction-to-Training-LLMs-Using-Reinforcement-Learning-From-Human-Feedback-RLHF---VmlldzozMzYyNjcy) by Ayush Thakur: 解释为什么 RLHF 需要减少 LLM 中的偏差并提高性能
* [Illustration RLHF](https://huggingface.co/blog/rlhf) by Hugging Face: RLHF 简介，包括奖励模型训练和强化学习微调
* [Preference Tuning LLMs](https://huggingface.co/blog/pref-tuning) by Hugging Face: 比较 DPO、IPO 和 KTO 算法以执行首选项对齐
* [LLM 训练：RLHF 及其替代方案](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives) by Sebastian Rashcka: RLHF 流程和 RLAIF 等替代方案概述
* [使用 DPO 微调 Mistral-7b](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html): 使用 DPO 微调 Mistral-7b 模型并重现 [NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B) 的教程。
