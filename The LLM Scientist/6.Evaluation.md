## 评估 LLM 是整个流程中被低估的部分，这既耗时又相对可靠。

你的下游任务应该决定你想要评估什么，但永远记住古德哈特定律：“当一个度量成为目标时，它就不再是一个好的度量。

* **传统指标**: 困惑度和 BLEU 分数等指标并不像以前那样受欢迎，因为它们在大多数情况下都存在缺陷。但了解它们以及何时可以应用它们仍然很重要。
* **General 基准**: 基于 [语言模型评估工具](https://github.com/EleutherAI/lm-evaluation-harness)，[Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) 是通用 LLM（如 ChatGPT）的主要基准。还有其他流行的基准测试，如 [BigBench](https://github.com/google/BIG-bench)、[MT-Bench](https://arxiv.org/abs/2306.05685) 等。
* **特定于任务的基准测试**: 摘要、翻译和问答等任务有专门的基准、指标，甚至子领域（医疗、金融等），例如用于生物医学问答的 [PubMedQA](https://pubmedqa.github.io/)
* **人工评估**: 最可靠的评估是用户的接受率或人类的比较。除了聊天跟踪之外，记录用户反馈（例如，使用 [LangSmith](https://docs.smith.langchain.com/evaluation/capturing-feedback)） 有助于确定需要改进的潜在领域

📚 **推荐课程**:
* [Perplexity of fixed-length models](https://huggingface.co/docs/transformers/perplexity) by Hugging Face: 使用 transformers 库实现
* [BLEU at your own risk](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213) by Rachael Tatman: BLEU 分数概述及其许多问题，并附有示例
* [LLM 评估调查](https://arxiv.org/abs/2307.03109) by Chang et al.: 关于评估内容、评估位置和评估方法的综合论文
* [Chatbot Arena 排行榜](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) by lmsys: 通用 LLM 的 Elo 评级，基于人类的比较


